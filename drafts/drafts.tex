\documentclass[12pt a]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{mathtools}
\usepackage{cancel}




\providecommand{\floor}[1]{\left \lfloor #1 \right \rfloor}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{ex}{Exercise}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{exercise}{Exercise}
\newtheorem{claim}[theorem]{Claim} 
\theoremstyle{definition}
\newtheorem{example}{Example}

%% environments
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{corollary}{Corollary}[theorem]
%\newtheorem{lemma}[theorem]{Lemma}
%\theoremstyle{definition}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\theoremstyle{definition}
%\newtheorem{example}{Example}
%\theoremstyle{definition}
%\newtheorem{fact}{Fact}
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}
%\theoremstyle{definition}
%\newtheorem{note}{Note}

\author{Jair}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Arithmetic Derivative}

\noindent
\textit{"Momma told me "you can't take derivatives on integers!!!" and I was like "hold my Leibniz rule""} 

\begin{definition}[\textbf{The Arithmetic Derivative for natural numbers}]
\phantom{} \\ 
For any $n \in \mathbb{N}_0$ the \textbf{arithmetic derivative}, denoted $(n)'$ is defined as follows: 
\begin{itemize}
	\item $(p)'=1$ , for any prime $p$
	\item $(pq)'= (p)'q + p(q)'$ , for any $p,q \in \mathbb{N}$ (\textbf{Leibniz Rule})
\end{itemize}

\end{definition}


\begin{corollary}[\textbf{Elementary derivatives}]
\phantom{} \\ 
$(0)'=0$ and $(1)'=0$
\end{corollary}

\begin{proof}
Follows immediately from the Leibniz rule with $p=q=1$
\end{proof}

\begin{corollary}[\textbf{Power Rule}]
For any integers $p$ and $n \geq 0$: 
$$
(p^n)'=np^{n-1}(p')
$$
\end{corollary}

\begin{proof}
Trivial
\end{proof}

\begin{corollary}[\textbf{Arithmetic derivatives of Integers and rational numbers}]
One can extend the arithmetic derivative to the integers by showing 
$$(-x)'=-(x)$$
Further, the \textbf{quotient rule} is also well defined on $\mathbb{Q}$: 

$$
\left(\frac{p}{q}\right)'
=
\left(\frac{(p)'q - p(q)'}{q^{2}}\right)
$$
\end{corollary}

\begin{proof}
Ain't nobody got da time fo' dis. 
\end{proof}


\begin{corollary}[\textbf{Prime factorization derivative formula}]
Let $\omega(x)$ be the prime omega function, indicating the number of distinct prime factors in $x$, and $\nu_{p}(x)$ be the p-adic valuation of $x$. Then, 
$$
(x)' = \sum _{\stackrel {p\mid x}{p{\text{ prime}}}}{\frac {v_{p}(x)}{p}}x
$$
\end{corollary}

\begin{proof}
The prime factorization of an integer $x\in \mathbb{Z}$ is given by 
$$
{\displaystyle x=\prod _{i=1}^{\omega (x)}{p_{i}}^{v_{p_{i}}(x)}}
$$
it follows that 
$$
\displaystyle D(x)=\sum _{i=1}^{\omega (x)}\left[v_{p_{i}}(x)\left(\prod _{j=1}^{i-1}{p_{j}}^{v_{p_{j}}(x)}\right)p_{i}^{v_{p_{i}}-1}\left(\prod _{j=i+1}^{\omega (x)}{p_{j}}^{v_{p_{j}}(x)}\right)\right]
$$
$$
=\sum _{i=1}^{\omega (x)}{\frac {v_{p_{i}}(x)}{p_{i}}}x=\sum _{\stackrel {p\mid x}{p{\text{ prime}}}}{\frac {v_{p}(x)}{p}}x
$$
\end{proof}


\begin{example}
$$
{\displaystyle (60)'=(2^{2}\cdot 3\cdot 5)'=\left({\frac {2}{2}}+{\frac {1}{3}}+{\frac {1}{5}}\right)\cdot 60=92,}
$$
$$
{\displaystyle (81)'=D(3^{4})=4\cdot 3^{3}\cdot D(3)=4\cdot 27\cdot 1=108.}
$$
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\title{Example}


Hello! \\
$$x+3= 67$$ \\
 
This is really fun!! 

$$ x + 35^2 = ??? $$
$$ 2x^{(35x+x^x)} $$
 
Greek letters : 

$$\pi $$
$$\beta $$ 
$$\alpha $$
$$ A = \pi r^2 $$
$$\log_5 (35)$${
	
fractions: 

$$ \frac{x}{y}$$
$$  \frac{x}{1+x+x^2} $$
$$\frac{\sqrt{x^2 + 1}}{\sqrt{x^2 -1 }} $$

brackets: 

$$ \{a,b,c\} $$
$$\left(\frac{m_3 + m _4}{x^2}\right) $$
$$\left| \frac{dx}{dy} \right|_{x=1}$$
$$\left| \frac{\delta x}{\delta y} \right|_{x=1}$$

$$ \forall n\in\mathbb{Z} \hspace{0.5cm} a-1| 	a^n-1 $$

$$\int x^{dx} - 1$$

This is a table \\

\begin{tabular}{|c|c|c|c|c|c|} %Take a look a the vertical lines
\hline % the hline draws the sides

$x$ & 1 & 2 & 3 & 4 & 5 \\ \hline 
$f(x)$ & 10 & 11 & 12 & 13 & 14\\  \hline

	
\end{tabular}

\begin{ex}
	Prove that if $p:prime$ , then $\sqrt{p}$ is irrational. More generally, prove that if $n\in \mathbb{Z}$ and $ n\neq m^2$, then $\sqrt{n}$ is irrational. 
\end{ex}
	

\begin{eqnarray*} %already in math mode 
	x &\approx 1.567 \\  % the & sign helps aligning the equal signs 
	4x+3^x &= &12
\end{eqnarray*}

Solve the following systems of congruences: 	
\begin{equation}
\begin{cases}
12x + 31y \equiv 2\text{ }(mod127)
\\
2x + 89y \equiv 23 \text{ }(mod127)
\end{cases}
\end{equation}	

\begin{equation}
\begin{cases}
x \equiv 1 \text{ }(mod3) \\ 
x \equiv 1 \text{ }(mod4) \\
x \equiv 1 \text{ }(mod5) \\ 
x \equiv 0 \text{ }(mod7) \\ 
\end{cases}
\end{equation}

Solve the following congruence polynomials:

\begin{equation}
x^2 \equiv 29 \text{ } (mod35)
\end{equation} 
\begin{equation}
3x^2 + 6x + 5 \equiv 0 \text{ } (mod7) 
\end{equation} 

Find $y$ such that the following holds:  
\begin{equation}
	19^{y^{1000}} \equiv 1 \text{ } mod(20) 
\end{equation}

Assume $p$ is prime, and $\exists a$ $1\leq a \leq(p-1)$ then

\begin{equation}
	\text{Show }[a],[2a],...,[(p-1)a] \text{ are all residue classes}
\end{equation}

Prove that if $n\in\mathbb{Z}$ and $n>0$, then the prime factorization of the binomial coefficient $C(n,k)$ is given by 

\begin{equation}
	{n\choose k} = \prod_{i=1}^{n} { {p_{k}}^{\sum_{i=1}^{n} \floor{\frac{n}{p^i_{k}}} -  
			\floor{\frac{n-k}{p^i_{k}}} -  
			\floor{\frac{k}{p^i_{k}}}}  }
\end{equation}

\begin{proof}
This is written in \LaTeX, so it must be true. 
\end{proof}

\newpage


\begin{thm}
P = NP
\end{thm}

\begin{proof}
Let $S =\{X | X \notin X  \}$, and let $NP \in S$. Since $P \subseteq{NP}$ then 
	$P\in P \iff P\notin P$, but clearly, we also have that $NP\in NP \iff NP\notin NP$. It follows trivially that $P\in NP \iff P\notin NP$ and $NP\in P \iff NP\notin P$. 
	Thus we conclude that $P=NP$
\end{proof}

\begin{corollary}
Where is my million dollars???
\end{corollary}

$$I_n = \begin{pmatrix} 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0\\ 0 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1
\end{pmatrix} \linebreak $$

$$I=(\delta_{ij})$$

$$A=U{\Sigma}V^{*}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Prove Mantel's Theorem, that is, prove that
\begin{equation}
	|E| \leq \frac{{|V}^2|}{4}
\end{equation}

\begin{theorem}[\textbf{LaPlace Expansion}]
	The determinant of an $n\times n$ matrix can be calculated as follows: 
	$$det(A)=\sum_{i=1}^{n} a_{i,j}Cof(A_{i,j})$$
	where 
	$$Cof(A_{i,j})=(-1)^{i+j}det\left( \underbracket[0.75pt]{A_{i,j}}_{\text{$(n-1) \times (n-1)$}} \right)$$
	and the expansion occurs along the i-th row. (Note the expansion can also be taken along the j-th column). 
\end{theorem}

If $A$ is an $n \times n$ matrix, and $E_1,E_2,...,E_k$ are elementary matrices resulting from taking $[A \overset{RREF}{\to} I_n]$ , then $A=E_1^{-1}E_2^{-1}\dots E_k^{-1}I_n$, and so
$$det(A)=det(E_1^{-1})\dots det(E_k^{-1})$$

\theoremstyle{definition}
\begin{exercise}
	Let \\ \\
	$A = \begin{bmatrix}
	a & d & 2 \\
	b & e & 1 \\ 
	c & f & 1 
	\end{bmatrix}$ , 
	$B = \begin{bmatrix}
	d & e & f \\
	a & b & c \\ 
	1 & 1 & 0 
	\end{bmatrix}$ , 
	$C = \begin{bmatrix}
	-a & 2c + 3a & 1 \\
	-b & 2e + 3b & 0 \\ 
	-c & 2f + 3c & 1  
	\end{bmatrix}$ \\ 
	
	If $det(A)=2$ and $det(B)=-3$, find $det[2A^{3}B^{-1}A^{T}adj(3C^2)]$
\end{exercise}

\begin{theorem}[\textbf{Inclusion-Exclusion for Probability}]
	The general inclusion-exclusion formula for the union of sets $A_1,A_2,...,A_n$ is completely determined by the simple formula
	$$\mathbb{P}\left(\bigcup_{i=1}^{n} A_{i} \right) = \mathbb{P}(\emptyset) -\sum_{n}^{k=1} 
	\left( \left[ sin\left( k\pi + \frac{\pi}{2} \right) \right]  \sum_{ 
		\substack{I\subset \{ n,,...,1 \} \\ |I|=k } } \left(1- \mathbb{P}\left(  \overline{ \bigcap_{i \in I} A_i} \right)  \right)  \right)$$
\end{theorem}
\begin{proof}
	The proof is trivial and left as an exercise for the reader.
\end{proof}


$$(\mathbf{w}^{*},\mathbf{\xi}^{*}, \alpha^{*}, \beta^{*}) = \underset{\mathbf{w}, \xi}{argmin} \text{\space} \underset{\alpha, \beta}{argmax} \mathcal{L}(\mathbf{w}, \mathbf{\xi}, \alpha , \beta)$$

$$=\underset{\mathbf{w}, \xi}{argmin} \text{\space} \underset{\alpha, \beta}{argmax} \text{\space} 
\frac{1}{2}\|\mathbf{w}\|  + 
C\sum_{i=1}^{n}\xi_{i} - 
\sum_{i=1}^{n}\alpha_{i}(y_{i}\mathbf{w}^{T}x_{i} - 1 + \xi_{i}) - 
\sum_{i=1}^{n}\beta_i \xi_i
$$


\begin{proposition}
Let $\delta^{\text{TM}}=(\Sigma, q, \Phi, \gamma, Q_O, q_{rEjEcT}, q_{ACCepT} )$ be a \textbf{Turing Machine}. you know the rest. 
\end{proposition}


$$\forall u \in U,\mathbf{w} \in u\mathbb{W} , U_{\mathbf{w}} \bigcup_{u \in \mathcal{U}} uWu \bigcup_{w \in \mathcal{W}}w_{u} := \mathcal{O} w \mathcal{O}$$


$$A=\{u1v:u,v\in\Sigma^*\text{ and }|u|,|v|\ge 1\}\;.$$

\newpage

\section{Gradient Descent for Linear Regression}

Suppose we have a hypothesis $h: \mathbb{R}^{n} \rightarrow  \mathbb{R}$ , $h_{\theta}(\textbf{x})=\hat{y}$ with paramters $\theta \in \mathbb{R}^{n}$. Recall the \textbf{Mean-Squared Loss} (MSE) metric, applied to linear regression:  

$$MSE(y,h_{\theta}(\textbf{x}))
=
\frac{1}{n}||\mathbf{y} - h_{\theta}(\textbf{x}) ||^{2}_2
=
\frac{1}{2n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2} 
= 
\frac{1}{2n}\sum_{i=1}^{n}(y^{(i)}-\mathbf{w}^{T}x^{(i)})^{2}$$
Then we have the general update: 

$$\mathbf{w}^{k+1}=\mathbf{w}^{k}+ \alpha \frac{\partial}{\partial \mathbf{w} }MSE(y,\hat{y}) $$

\textbf{Batch Gradient Descent:} For $k=0,1,\dots$

\begin{enumerate}
	\item For $k=0,1,\dots$
	$$\mathbf{w}^{k+1}=\mathbf{w}^{k}+ \alpha 
\frac{1}{n}\sum_{i=1}^{n}(y^{(i)}-\mathbf{w}^{T}x^{(i)})\mathbf{x}^{(i)} $$
\end{enumerate}



\textbf{Mini-batch Gradient Descent:}  

\begin{enumerate}
	\item For $k=0,1,\dots$ 
	\begin{enumerate}
		\item Split data $D$ into $T$ subsets $D_{t}$ of sizes $n_{0},\dots,n_{T-1}$, s.t. $\sum_{t}n_{t}=1$. 
		\item For each subset $D_{t}$: 		
		
	$$\mathbf{w}:=\mathbf{w}+ \alpha 
\frac{1}{n_{t}}\sum_{i: x^{(i)} \in D_{t}}^{n_{t}}(y^{(i)}-\mathbf{w}^{T}x^{(i)})\mathbf{x}^{(i)} $$
		
	\end{enumerate}	 
\end{enumerate} 



\textbf{Stochastic Gradient Descent:}  

\begin{enumerate}
	\item For $k=0,1,\dots$ 
	\begin{enumerate}
		\item For $i=1,\dots,n$: 		
	$$\mathbf{w}:=\mathbf{w}+ \alpha 
(y^{(i)}-\mathbf{w}^{T}x^{(i)})\mathbf{x}^{(i)} $$
		
	\end{enumerate}	 
\end{enumerate} 

\newpage

\textbf{L1-norm}

$$\|\mathbf{w}\|_{1} = \sum_{i}|w_{i}| $$

$$\frac{\partial}{\partial \mathbf{w}} \|\mathbf{w}\|_{1} = sign(\mathbf{w}) = [sign(w_{1}),\dots,sign(w_{m})]^{T}$$

$$C = \sum_{w \in LDA(D)} E[idx(w)]$$

\newpage



\begin{theorem}[\textbf{Inclusion-Exclusion in Measure Theory}] 
Let $(X,\mu)$ be  a finite measure space. For any finite measurable sets $A_{1},\dots,A_{n} \subseteq X$

$$
\mu \left(\bigcup _{i=1}^{n}A_{i}\right)
$$

$$
= \sum _{i=1}^{n}\mu(A_{i})-\sum _{1\leqslant i<j\leqslant n}\mu(A_{i}\cap A_{j})+\sum _{1\leqslant i<j<k\leqslant n}\mu(A_{i}\cap A_{j}\cap A_{k})-\cdots +(-1)^{n-1}\mu(A_{1}\cap \cdots \cap A_{n})
$$

$$
= {\displaystyle \mu \left(\bigcup _{i=1}^{n}A_{i}\right)=\sum _{\emptyset \neq J\subseteq \{1,\ldots ,n\}}(-1)^{|J|+1} \mu \left(\bigcap _{j\in J}A_{j}\right).}
$$

$$
= {\displaystyle \mu \left(\bigcup _{i=1}^{n}A_{i}\right)=\sum _{k=1}^{n}\left((-1)^{k-1}\sum _{I\subseteq \{1,\ldots ,n\} \atop |I|=k} \mu(A_{I})\right),}
$$

$$
= \mu(\emptyset) -\sum_{n}^{k=1} 
	\left( \left[ sin\left( k\pi + \frac{\pi}{2} \right) \right]  \sum_{ 
		\substack{I\subset \{ n,...,1 \} \\ |I|=k } } \left(1- \mu \left(  \overline{ \bigcap_{i \in I} A_i} \right)  \right)  \right)
$$

\begin{proof}
Yikes
\end{proof}

\end{theorem}


\begin{theorem}[\textbf{Inclusion-Exclusion}] 
Given $n$ sets $A_{1},\dots,A_{n}$ in an universal space $S$, the cardinality of the union of $n$ such sets is given by 
$$
\left|\bigcup _{i=1}^{n}A_{i}\right|
$$

$$
= \sum _{i=1}^{n}|A_{i}|-\sum _{1\leqslant i<j\leqslant n}|A_{i}\cap A_{j}|+\sum _{1\leqslant i<j<k\leqslant n}|A_{i}\cap A_{j}\cap A_{k}|-\cdots +(-1)^{n-1}\left|A_{1}\cap \cdots \cap A_{n}\right|
$$

$$
= {\displaystyle \left|\bigcup _{i=1}^{n}A_{i}\right|=\sum _{\emptyset \neq J\subseteq \{1,\ldots ,n\}}(-1)^{|J|+1}\left|\bigcap _{j\in J}A_{j}\right|.}
$$

$$
= {\displaystyle  \left|\bigcup _{i=1}^{n}A_{i}\right|=\sum _{k=1}^{n}\left((-1)^{k-1}\sum _{I\subseteq \{1,\ldots ,n\} \atop |I|=k} |A_{I}|\right),}
$$

$$
= |\emptyset| -\sum_{n}^{k=1} 
	\left( \left[ sin\left( k\pi + \frac{\pi}{2} \right) \right]  \sum_{ 
		\substack{I\subset \{ n,...,1 \} \\ |I|=k } } \left(1- \left|  \overline{ \bigcap_{i \in I} A_i} \right|  \right)  \right) 
$$


\end{theorem}

\begin{proof}
Yikes
\end{proof}




\newpage

\begin{claim} 
The Post-Correspondence Problem $PCP$ is decidable relative to the acceptance problem $A_{TM}$. 
\end{claim}

\begin{proof}
We essentially want to show $PCP$ is reducible to $ATM$, i.e., if $ATM$ were decidable, then the $PCP$ problem would also be decidable. For this purpose, we suppose $A_{TM}$ were decidable, and use the fact that mapping reducibility is transitive. We first reduce $HALT_{TM}$ to $A_{TM}$. If $A_{TM}$ were decidable, $HALT_{TM}$ would also be decidable. Let $S$ be Turing Machine deciding $A_{TM}$. We construct the T.M. $H$ for $HALT_{TM}$ as follows: 

$H$ = "On input $<M,x>$, where $M$ is a Turing Machine and $w$ its input:

\begin{enumerate}
	\item Construct machine $M'$ from $M$ by marking all rejecting states of $M$ as accepting. 
	\item Run $S$ on $<M',x>$, if it accepts, \textbf{accept}, if it rejects $\textbf{reject}$. "
\end{enumerate} 

Clearly, $<M,x> \in HALT_{TM}$ if $<M',w> \in A_{TM}$. Let this machine $H$ also be an oracle T.M. Now, by using the transitivity of mapping reduction, if $A_{TM}$ were decidable, then $HALT_{TM}$ would also be decidable, so we can show that if $HALT_{TM}$ were decidable, $PCP$ would also be. Now let $R$ be a Turing Machine that recognizes $PCP$, (which checks in linear time whether a proposed match is an actual solution to $PCP$ by simply comparing the top and bottom symbols and accepting if all of them are equal and rejecting otherwise). Assume that $s_{1},s_{2},\dots$ is a list of all possible strings in $PCP$. Construct the following enumerator $E$. 	

$E$ = "Ignore the input. 

\begin{enumerate}
	\item Repeat the following for $i=1,2,3,\dots$ 
	\item	Run $R$ for $i$ steps on each input $s_{1},s_{2},\dots, s_{i}$. 
	\item	If any computations accept, print out the corresponding $s_{j}$."
\end{enumerate}

If $R$ accepts a particular string $s$, eventually it will appear on the list generated by $E$. Finally, we construct the decider $P$ for $PCP$: 

$P$ = "On input $<R,x>$, where $R$ is the machine described above: 

\begin{enumerate}
	\item Query the oracle for $H$ on input $<E,x>$. 
	\item If $E \in HALT_{TM}$, a solution to PCP exists, so run $E$ until the solution is out and \textbf{accept}. 
	\item If $E \notin HALT_{TM}$, no solution to PCP exists, so \textbf{reject}. "
\end{enumerate}

This shows that $PCP$ is decidable relative to $A_{TM}$

\end{proof}

\newpage 


\begin{claim}
Use the languages $A={a^{m}b^{n}c^{n}|m,n \geq 0}$ together with example 2.36 to show that the class of context-free languages is not closed under intersection. 
\end{claim} 

\begin{proof}
Note that  

$$L(A\cap B) = {w|w = a^{m}b^{n}c^{n} \wedge
}$$
\end{proof}


$$
C = \sum_{w \in D, tfidf(w)>t} E[idx(w)] $$
$$\overset{\text{Proposed change}}{\implies}$$
$$ C = \sum_{w \in LDA(D)} E[idx(w)]
$$

$$
\int x^{dx} - 1
$$

$$
\int \frac{x^{dx} - 1}{dx} dx 
=
\int \underset{\Delta x \rightarrow 0}{lim} \frac{x^{\Delta x} - 1}{\Delta x} dx
$$


\begin{theorem}[\textbf{Freshman's Dream}]
\begin{multline} 
\phantom{}  
\text{ \textit{Roses are red}} \\ 
\text{ \textit{Violets are blue}} \\ 
(x+y)^{n} = x^n + y^n \text{ \textit{is true}} \\  
\text{\textit{in }} \mathbb{Z}/2\mathbb{Z}  
\end{multline}
\end{theorem}


\begin{theorem}[\textbf{Fubini's Theorem}]
Let $X$ and $Y$ be $\sigma$-finite measure spaces, and suppose $X \times Y$ is the given product measure. Then, if $f$ is a $X \times Y$ (measurable) function and 
$$
\int_{X \times Y} |f(x,y)| d(x,y) < \infty
$$ 
then 
$$
\int_{X} \left( \int_{Y} f(x,y) dy \right) dx = 
\int_{Y} \left( \int_{X} f(x,y) dx \right) dy
=
\int_{X \times Y} f(x,y) d(x,y)
$$ 
\end{theorem}

\newpage

\begin{theorem}[\textbf{Freshman's dream}]
Let $n$ be prime, then 

$$
(x+y)^{n} = x^{n} + y^{n} 
$$ 

holds in $\mathbb{Z}/n \mathbb{Z}$
\end{theorem}

\begin{proof}
Let $p$ be a prime, and note that 

$$
(x+y)^{p} = 
x^{p} + {p \choose 1}x^{p-1}y + {p \choose 2}x^{p-2}y^{2} + \dots  +  {p \choose p-1}x^{p-1}y + y^{p}
$$

Let $1 \leq i \leq p-1$, and consider 

$$
{p \choose i } = \frac{p!}{i!(p-1)!} = p \frac{(p-1)!}{i! (p-i!)} \in \mathbb{N}
$$

\noindent
Any factor from $i!(p-i)!$ is not going to divide $p$. \\
Why is this is true? We know that 

$$
i!=1*2*\dots*i <p
$$
$$
(p-1)!=1*2*\dots*(p-1) <p
$$

\noindent
which implies that none of the factors above can divide $p$ $\implies$ any of them must divide $(p-i)!$. 

$$
\implies {p\choose i} = p*k_i \in \mathbb{N} 
\implies p\;|{p\choose i} \;, i=1,\dots,(p-1)
$$
$$
\therefore (x+y)^p \equiv x^p + y^p \; mod \; p
$$

\end{proof}


$$
J(\Theta) 
=
-\frac{1}{n} \left[ \sum_{i=1}^{n} \sum_{k=1}^{K} y_{k}^{(i)} \log h_{\theta}(x^{(i)})_{k} 
+ (1 - y_{k}^{(i)}) \log (1 - h_{\theta}(x^{(i)})_{k} ) \right] 
+ 
\frac{\lambda}{2n} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1} ^{s_l + 1} (\Theta_{j}^{l})^{2}
$$


$$
\frac{d^2 y}{dx^2} = \left( \frac{dy}{dx} \right)^2
$$


$$
\underset{\Delta x \rightarrow 0}{lim} \int_{-1}^{0} \left( \frac{1}{x^2} + \mathbb{B}ruh \right) \Delta x 
$$


\newpage

$$
\int_{a}^{b}f(x)dx = -\int_{b}^{a}f(x)dx 
$$

$$
\implies 
\underset{N \rightarrow \infty }{lim} \sum_{i=1}^{N} f(x_{i}^{*}) \Delta x
= 
\underset{N \rightarrow \infty }{lim} \sum_{i=1}^{N} f(x_{i}^{*}) \left(\frac{b-a}{N}\right) 
= 
-\underset{N \rightarrow \infty }{lim} \sum_{i=1}^{N} f(x_{i}^{*}) \left(\frac{a-b}{N}\right) 
$$
$$
\implies 
\sum_{i=1}^{N} f(x_{i}^{*}) \left(\frac{b-a}{N}\right) 
= 
- \sum_{i=1}^{N} f(x_{i}^{*}) \left(\frac{a-b}{N}\right) 
:= 
- \sum_{N}^{i=1} f(x_{i}^{*}) \left(\frac{b-a}{N}\right)
$$


\noindent
Define 
$$
\sum_{N}^{n=1} a_n  := -a_N - a_{N-1} - \dots - a_1
$$

\noindent
Then 
$$
\sum_{n=1}^{N} a_n = - \sum_{N}^{n=1} a_n
$$

\noindent
This is well-defined as it is no more than a special case of the definite integral : 
$$
\sum_{k=a}^{b} f(k) = \int_{[a,b]} f d\mu
$$
where $\mu$ is the \textbf{counting measure}. 



$$
\int_{0}^{\phi=y} exp\left\{\displaystyle \int_{-\infty}^{x=t} e^{x} dx \right\} d\phi = ye^{ \displaystyle e^{\displaystyle t}}
$$

\newpage

\begin{exercise}
Let $x\in \mathbb{R}$, $N \in \mathbb{Z}$, $m \in \mathbb{N}$. Find the exact value of
$$
\underset{m\text{-times}}{\underbrace{\displaystyle \sqrt[N]{x \sqrt[N]{x \sqrt[N]{x \dots \sqrt[N]{x}} }}
}}
$$
\begin{enumerate}
	\item When $m < \infty$ 
	\item Show that the expression above converges when $m \rightarrow \infty$ and find describe the conditions for convergence on the value of $x$ and $N$. 
\end{enumerate}
\end{exercise}

\begin{proof}
Note that
$$\sqrt[N]{x} = x^{\frac{1}{N}} \;\;\;\; (m=1)$$
$$\sqrt[N]{x \sqrt[N]{x} } = \sqrt[N]{x * x^{\frac{1}{N}} } = x^{\frac{1}{N}} * x^{\frac{1}{N^{2}}} \;\;\;\; (m=2)$$
$$\sqrt[N]{x \sqrt[N]{x \sqrt[N]{x}} } = \sqrt[N]{x * x^{\frac{1}{N}} * x^{\frac{1}{N^{2}}} } = x^{\frac{1}{N}} * x^{\frac{1}{N^{2}}} * x^{\frac{1}{N^3}} \;\;\;\; (m=3)$$
$$\vdots$$
$$\sqrt[N]{x \sqrt[N]{x \sqrt[N]{x \dots \sqrt[N]{x}}}}
= \prod_{i=1}^{m}x^{1/N^{i}} \;\;\;\; (m=m)$$

But this is simply 

$$
\prod_{i=1}^{m}x^{1/N^{i}} 
= 
x^{\displaystyle \sum_{i=1}^{m} \dfrac{1}{N^{i}} }
$$

When $m < \infty$ , when $N \neq 1 \implies \frac{1}{N} \neq 1$

$$
\sum_{i=1}^{m} \dfrac{1}{N^{i}}
\overset{i=j+1}{=} 
\sum_{j=0}^{m-1} \dfrac{1}{N^{i+1}}
= 
\frac{1}{N}\sum_{j=0}^{m-1} \dfrac{1}{N^{i}}
= 
\frac{1}{N}\left( \dfrac{1-\left(\dfrac{1}{N}\right)^{m}}{1 - \dfrac{1}{N}} \right)
$$

Further, when $m \rightarrow \infty$, as for $|N|>1$,  $\left|\dfrac{1}{N}\right| < 1$


$$
\underset{m \rightarrow \infty}{\lim} \sum_{i=1}^{m} \dfrac{1}{N^{i}}
= 
\dfrac{1}{N} \sum_{i=1}^{\infty} \dfrac{1}{N^{i-1}}
= 
\dfrac{1}{N} \sum_{i=1}^{\infty} \left( \dfrac{1}{N} \right)^{i-1}
= 
\dfrac{1}{N} \left(\dfrac{1}{1-\dfrac{1}{N}}\right)
$$

So that 

\begin{equation}
\underset{m\text{-times}}{\underbrace{\displaystyle \sqrt[N]{x \sqrt[N]{x \sqrt[N]{x \dots \sqrt[N]{x}} }}
}}
= \prod_{i=1}^{m}x^{1/N^{i}} 
= x^{\displaystyle \sum_{i=1}^{m} \dfrac{1}{N^{i}} }
=
x^{ \left\{ \displaystyle \frac{1}{N}\left( \dfrac{1-\left(\dfrac{1}{N}\right)^{m}}{1 - \dfrac{1}{N}} \right) \right\} }
\end{equation}

and  

\begin{equation}
\underset{m \rightarrow \infty}{\underbrace{\displaystyle \sqrt[N]{x \sqrt[N]{x \sqrt[N]{x \dots \sqrt[N]{x}} }}
}}
= \prod_{i=1}^{\infty}x^{1/N^{i}} 
= x^{\displaystyle \sum_{i=1}^{\infty} \dfrac{1}{N^{i}} }
=
x^{ \left\{ \displaystyle  \dfrac{1}{N} \left(\dfrac{1}{1-\dfrac{1}{N}}\right)  \right\} } 
\end{equation}

\end{proof}


\begin{corollary}[\textbf{Infinite nested square root of 2}] 
$$
\underset{m \rightarrow \infty}{\underbrace{\displaystyle \sqrt[]{2 \; \sqrt[]{2 \; \sqrt[]{2 \; \dots \sqrt[]{2}} }}
}}
= 2 
$$
\end{corollary}

\begin{proof}
Trivial y the formula above with $x=2$ , $N=2$
\end{proof}



\textit{Excuse me sir, do you have a moment to talk about our God and saviour }

$$\text{\LaTeX} ?$$ 


$$
y_{+++} = \sum_{I=1}^I \sum_{j=1}^J \sum_{k=1}^K y_{ijk}
$$

\newpage

\begin{theorem}
Let $D = diag(d_1,\dots,d_m) \in \mathbb{F}^{m \times m}$ de a real or complex-valued matrix. Then 

$$
e^{D} = 
\begin{pmatrix}
e^{d_1}    &      &     &    &    \\ 
  & \ddots &      &     &   \\ 
  &  & e^{d_j}    &     &     \\ 
  &        &      & \ddots  &     \\ 
  &  &      &     & e^{m}  \\ 
\end{pmatrix}
$$
\end{theorem}


\begin{proof}
We have that  

\begin{eqnarray}
e^{D} &=& \boldsymbol{I}_{m} e^{D} \\ 
&=& \boldsymbol{I}_{m} \sum_{n=0}^{\infty} \frac{1}{n!} D^{n} \\ 
&=& \sum_{n=0}^{\infty} \frac{1}{n!} \left( \sum_{j} [\boldsymbol{I}_{m}]_{ij}  D_{jk} \right)^{n}
\end{eqnarray}

Since  

$$
[\boldsymbol{I}_{m}]_{ij}  D_{jk} 
= 
\begin{cases}
D_{jk} \;\;\; \forall \;\;\; i=j, j=k \\ 
0 \;\;\; \text{else}
\end{cases} 
$$

\begin{eqnarray}
\implies
e^{D} &=& \sum_{n=0}^{\infty} \frac{1}{n!} \left( \sum_{j}  D_{jj} \right)^{n} \\
&=& 
\sum_{n=0}^{\infty} \frac{1}{n!} \left( \sum_{j}   d_j \right)^{n} \\ 
&=& 
e^{\sum_{j}d_j}
\end{eqnarray}

\begin{eqnarray}
\therefore 
e^{D} &=& 
\boldsymbol{I}_{m} e^{D} \\
&=& 
\boldsymbol{I}_{m} e^{\sum_{j}d_j} \\
&=& 
\begin{pmatrix}
e^{d_1}    &      &     &    &    \\ 
  & \ddots &      &     &   \\ 
  &  & e^{d_j}    &     &     \\ 
  &        &      & \ddots  &     \\ 
  &  &      &     & e^{m}  \\ 
\end{pmatrix}
\end{eqnarray}



\end{proof}















\end{document}


